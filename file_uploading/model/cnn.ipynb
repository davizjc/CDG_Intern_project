{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6CGRF5Xqx6n",
        "outputId": "d31dd21d-64b3-4b57-ac65-6ca87ad11f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVce6Ezz7QEW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import date\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-9_9g9o7cWt"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, sizeX:tuple, sizeY:tuple):\n",
        "        super().__init__()\n",
        "        self._sizeX = sizeX  # input\n",
        "        self._sizeY = sizeY  # output\n",
        "        self._model = self._BuildModel()\n",
        "        self._learner = self._BuildLearner()\n",
        "\n",
        "    def call(self, x:tf.Tensor, training:bool=False) -> tf.Tensor:\n",
        "        prediction = self._model(x, training=training)\n",
        "        return prediction\n",
        "\n",
        "    @tf.function\n",
        "    def Train(self, x:tf.Tensor, y:tf.Tensor): #update the model's weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = self.__call__(x)\n",
        "            loss = self._learner[\"get_loss\"](prediction, y)\n",
        "        gradient = tape.gradient(loss, self._model.trainable_variables)\n",
        "        self._learner[\"optimize\"].apply_gradients(zip(gradient, self._model.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def Validate(self, x:tf.Tensor, y:tf.Tensor) -> tf.Tensor: #evaluating the model's performance\n",
        "        prediction = self.__call__(x, training=False)\n",
        "        review = tf.math.in_top_k(tf.math.argmax(y,axis=1), prediction, 1)\n",
        "        accuracy = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
        "        return accuracy\n",
        "\n",
        "    def _BuildModel(self) -> tf.keras.Model:  # 8 layer doesn't include input layer(else 9)\n",
        "        tensorInput = tf.keras.Input(shape=self._sizeX) #input layer\n",
        "        featureMaps = tf.keras.layers.Lambda(lambda x: x/127.5-1.0)(tensorInput) #This is a Lambda layer that normalizes the input data.\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=16, kernel_size=[3,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=32, kernel_size=[1,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=64, kernel_size=[1,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=128, kernel_size=[1,3], activation=\"relu\")(featureMaps) #capture more complex patterns in the later stages of the network\n",
        "        featureMaps = tf.keras.layers.GlobalAveragePooling2D()(featureMaps) # applies Global Average Pooling, help reduces the number of parameters and helps to prevent overfitting.\n",
        "        featureMaps = tf.keras.layers.Dropout(0.5)(featureMaps ) #prevent overfitting,\n",
        "\n",
        "        result = tf.keras.layers.Dense(units=self._sizeY[0], activation=\"softmax\")(featureMaps) #output layers, softmax activation function is used to convert the outputs into probability scores for each class, making it suitable for multi-class classification tasks.\n",
        "        model = tf.keras.Model(tensorInput, result) #takes tensorInput as input and gives result as output\n",
        "        return model\n",
        "\n",
        "    def _BuildLearner(self) -> dict:\n",
        "        loser = lambda p, y: tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(p+1e-13),axis=1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        learner = {\"get_loss\": loser, \"optimize\": optimizer}\n",
        "        return learner\n",
        "        #creates a loss function\n",
        "        #optimizer (Adam with a learning rate of 1e-3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AisBg2Iq7v9l"
      },
      "outputs": [],
      "source": [
        "def PrepareDataset(dataDir:str, batchSize:int=10) -> dict:\n",
        "    dataDirDict = {\"train\":pathlib.Path(dataDir)/\"Train\", \"valid\":pathlib.Path(dataDir)/\"Valid\"}\n",
        "\n",
        "    classes = sorted(['passport','national_id_card', 'house_register', 'driver_license','commercial_registration'])\n",
        "    classIdxDict = dict(zip(classes,range(len(classes))))\n",
        "\n",
        "    paths = {\"train\":list(), \"valid\":list()}\n",
        "    labels = {\"train\":list(), \"valid\":list()}\n",
        "    for eachSet, eachDir in dataDirDict.items():\n",
        "        for eachPath in eachDir.rglob(\"*\"):\n",
        "            if eachPath.is_file():\n",
        "                paths[eachSet].append(str(eachPath))\n",
        "                labels[eachSet].append(classIdxDict[eachPath.parts[-2]])\n",
        "\n",
        "    dataset = dict()\n",
        "    decoder = lambda x, y: [tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x), channels=3), [224, 224]), tf.one_hot(y, len(classIdxDict))]\n",
        "    for eachSet in dataDirDict.keys():\n",
        "        eachTFData = tf.data.Dataset.from_tensor_slices((paths[eachSet],labels[eachSet]))\n",
        "        dataset.update({eachSet:eachTFData})\n",
        "        dataset[eachSet] = dataset[eachSet].shuffle(len(paths[eachSet]), reshuffle_each_iteration=True)\n",
        "        dataset[eachSet] = dataset[eachSet].map(decoder, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset[eachSet] = dataset[eachSet].batch(batchSize, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14JM5ymX-I-n"
      },
      "source": [
        "# Fine tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXPrmJBn7cZF",
        "outputId": "5becddbe-d55c-4a67-b525-5222ac7ad666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n",
            "Build the CNN model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_902) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "Epoch: 0,    Train perf: 99.44,    Valid perf: 95.00\n",
            "Epoch: 1,    Train perf: 97.78,    Valid perf: 92.50\n",
            "Epoch: 2,    Train perf: 96.67,    Valid perf: 90.00\n",
            "Epoch: 3,    Train perf: 97.78,    Valid perf: 90.00\n",
            "Epoch: 4,    Train perf: 89.44,    Valid perf: 82.50\n",
            "Epoch: 5,    Train perf: 89.44,    Valid perf: 90.00\n",
            "Epoch: 6,    Train perf: 99.44,    Valid perf: 95.00\n",
            "Epoch: 7,    Train perf: 96.11,    Valid perf: 92.50\n",
            "Epoch: 8,    Train perf: 99.44,    Valid perf: 92.50\n",
            "Epoch: 9,    Train perf: 89.44,    Valid perf: 92.50\n",
            "Epoch: 10,    Train perf: 99.44,    Valid perf: 87.50\n",
            "Epoch: 11,    Train perf: 93.89,    Valid perf: 90.00\n",
            "Epoch: 12,    Train perf: 97.78,    Valid perf: 92.50\n",
            "Epoch: 13,    Train perf: 96.11,    Valid perf: 95.00\n",
            "Epoch: 14,    Train perf: 90.56,    Valid perf: 87.50\n",
            "Epoch: 15,    Train perf: 97.22,    Valid perf: 92.50\n",
            "Epoch: 16,    Train perf: 99.44,    Valid perf: 92.50\n",
            "Epoch: 17,    Train perf: 98.89,    Valid perf: 92.50\n",
            "Epoch: 18,    Train perf: 97.22,    Valid perf: 92.50\n",
            "Epoch: 19,    Train perf: 94.44,    Valid perf: 87.50\n",
            "Epoch: 20,    Train perf: 98.89,    Valid perf: 92.50\n",
            "Epoch: 21,    Train perf: 97.22,    Valid perf: 87.50\n",
            "Epoch: 22,    Train perf: 97.22,    Valid perf: 92.50\n",
            "Epoch: 23,    Train perf: 96.11,    Valid perf: 85.00\n",
            "Epoch: 24,    Train perf: 97.22,    Valid perf: 92.50\n",
            "Epoch: 25,    Train perf: 100.00,    Valid perf: 92.50\n",
            "Epoch: 26,    Train perf: 100.00,    Valid perf: 87.50\n",
            "Epoch: 27,    Train perf: 100.00,    Valid perf: 95.00\n",
            "Epoch: 28,    Train perf: 100.00,    Valid perf: 87.50\n",
            "Epoch: 29,    Train perf: 93.89,    Valid perf: 92.50\n",
            "Epoch: 30,    Train perf: 98.33,    Valid perf: 92.50\n",
            "Epoch: 31,    Train perf: 98.89,    Valid perf: 92.50\n",
            "Epoch: 32,    Train perf: 100.00,    Valid perf: 92.50\n",
            "Epoch: 33,    Train perf: 99.44,    Valid perf: 92.50\n",
            "Epoch: 34,    Train perf: 100.00,    Valid perf: 87.50\n",
            "Epoch: 35,    Train perf: 98.89,    Valid perf: 92.50\n",
            "Epoch: 36,    Train perf: 96.67,    Valid perf: 92.50\n",
            "Epoch: 37,    Train perf: 100.00,    Valid perf: 90.00\n",
            "Epoch: 38,    Train perf: 99.44,    Valid perf: 90.00\n",
            "Epoch: 39,    Train perf: 99.44,    Valid perf: 92.50\n",
            "Epoch: 40,    Train perf: 98.33,    Valid perf: 90.00\n",
            "Epoch: 41,    Train perf: 93.89,    Valid perf: 82.50\n",
            "Epoch: 42,    Train perf: 94.44,    Valid perf: 90.00\n",
            "Epoch: 43,    Train perf: 95.56,    Valid perf: 90.00\n",
            "Epoch: 44,    Train perf: 99.44,    Valid perf: 87.50\n",
            "Epoch: 45,    Train perf: 97.78,    Valid perf: 92.50\n",
            "Epoch: 46,    Train perf: 100.00,    Valid perf: 90.00\n",
            "Epoch: 47,    Train perf: 99.44,    Valid perf: 92.50\n",
            "Epoch: 48,    Train perf: 99.44,    Valid perf: 87.50\n",
            "Epoch: 49,    Train perf: 100.00,    Valid perf: 92.50\n",
            "Completed!\n"
          ]
        }
      ],
      "source": [
        "if __name__== \"__main__\":\n",
        "    print(\"Preparing dataset...\")\n",
        "    dataset = PrepareDataset(r\"/content/drive/MyDrive/doc\", batchSize=10)\n",
        "    print(\"Build the CNN model...\")\n",
        "    newModel = tf.keras.models.load_model('/content/drive/MyDrive/code/model_up3')\n",
        "    # myModel = MyModel(sizeX=(224, 224, 3), sizeY=(5,))\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    best_valid_perf = 97.50\n",
        "    best_train_perf = 97.22\n",
        "    for epoch in range(50):\n",
        "        perfDict = {\"train\":[], \"valid\":[]}\n",
        "        for inData, outData in dataset[\"train\"]:\n",
        "            newModel.Train(inData, outData)\n",
        "        for inData, outData in dataset[\"train\"]:\n",
        "            perfDict[\"train\"].append(newModel.Validate(inData, outData))\n",
        "        for inData, outData in dataset[\"valid\"]:\n",
        "            perfDict[\"valid\"].append(newModel.Validate(inData, outData))\n",
        "        trainPerf = tf.math.reduce_mean(perfDict[\"train\"]) * 100\n",
        "        validPerf = tf.math.reduce_mean(perfDict[\"valid\"]) * 100\n",
        "        if validPerf > best_valid_perf and trainPerf > best_train_perf:\n",
        "            best_valid_perf = validPerf\n",
        "            best_train_perf = trainPerf\n",
        "            newModel.save('/content/drive/MyDrive/code/model_up3',save_format='tf')\n",
        "            print('savee model')\n",
        "        print(f\"Epoch: {epoch},    Train perf: {trainPerf:.2f},    Valid perf: {validPerf:.2f}\")\n",
        "    print(\"Completed!\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6rTta4RKVD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99d993d-3f94-4159-e542-9f25aa843eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall train accuracy: 99.44\n",
            "Overall valid accuracy: 90.00\n"
          ]
        }
      ],
      "source": [
        "# Evaluate overall training accuracy\n",
        "train_accuracy = []\n",
        "for inData, outData in dataset[\"train\"]:\n",
        "    train_accuracy.append(newModel.Validate(inData, outData))\n",
        "overall_train_accuracy = tf.reduce_mean(train_accuracy)\n",
        "\n",
        "# Evaluate overall validation accuracy\n",
        "valid_accuracy = []\n",
        "for inData, outData in dataset[\"valid\"]:\n",
        "    valid_accuracy.append(newModel.Validate(inData, outData))\n",
        "overall_valid_accuracy = tf.reduce_mean(valid_accuracy)\n",
        "\n",
        "print(f\"Overall train accuracy: {overall_train_accuracy*100:.2f}\")\n",
        "print(f\"Overall valid accuracy: {overall_valid_accuracy*100:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mkYMC8gJhYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04af4d3-c674-4d61-e0ff-76a4d243129d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting db-sqlite3\n",
            "  Downloading db-sqlite3-0.0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting db (from db-sqlite3)\n",
            "  Downloading db-0.1.1.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antiorm (from db->db-sqlite3)\n",
            "  Downloading antiorm-1.2.1.tar.gz (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: db-sqlite3, db, antiorm\n",
            "  Building wheel for db-sqlite3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db-sqlite3: filename=db_sqlite3-0.0.1-py3-none-any.whl size=1769 sha256=77ecf8a1ca0935f81936407ca87bc5d07b6246635603db4ce8909f475a67048e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/b7/83/e941e0a0e04f417982e718ae7295d1e82b5f2863a1c51edd71\n",
            "  Building wheel for db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db: filename=db-0.1.1-py3-none-any.whl size=3874 sha256=715c6bd32bd99521228197d53130533b5d682c6989734bcba7deaeda4d2f2f54\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/e4/df/bc55b93af204ab098d9effec76f6889ad12d7ad74e833c4910\n",
            "  Building wheel for antiorm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antiorm: filename=antiorm-1.2.1-py3-none-any.whl size=31664 sha256=b8cc2abb8baa1847835e00baef80c22d128736fd8be83e591293bd22eccd8e62\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/9f/7e/b7c95b391cfa77a9e722d359e9c669cf6c8d798d748aec5091\n",
            "Successfully built db-sqlite3 db antiorm\n",
            "Installing collected packages: antiorm, db, db-sqlite3\n",
            "Successfully installed antiorm-1.2.1 db-0.1.1 db-sqlite3-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install db-sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3XWKLkOdqor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912fd5b1-abb8-4c80-df14-86109de304a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_197475) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.99982494\n",
            "The image dl1.jpg is classified as driver_license\n",
            "0.9981687\n",
            "The image dl2.JPG is classified as driver_license\n",
            "0.9989472\n",
            "The image dl3.JPG is classified as driver_license\n",
            "0.6613281\n",
            "The image dl4.JPG is classified as driver_license\n",
            "0.999946\n",
            "The image dl5.JPG is classified as driver_license\n",
            "0.9998022\n",
            "The image dl6.JPG is classified as driver_license\n",
            "0.99721944\n",
            "The image dl7.JPG is classified as driver_license\n",
            "0.99992704\n",
            "The image dl9.JPG is classified as driver_license\n",
            "0.9971193\n",
            "The image dl10.JPG is classified as driver_license\n"
          ]
        }
      ],
      "source": [
        "def classify_image(image_path, model, class_names, confidence_threshold=0.6):\n",
        "    # Convert image_path to string\n",
        "    image_path = str(image_path)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    img = img[None, ...]  # Add a batch dimension\n",
        "\n",
        "    # Pass the image through the model and get the predicted probabilities\n",
        "    predictions = model(img)\n",
        "    predicted_class_index = np.argmax(predictions, axis=-1)\n",
        "    predicted_probability = np.max(predictions)\n",
        "    print(predicted_probability)\n",
        "\n",
        "\n",
        "    # Check if the predicted probability is below the threshold\n",
        "    if predicted_probability < confidence_threshold:\n",
        "        return 'other'\n",
        "    else:\n",
        "        return class_names[predicted_class_index[0]]\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    db_file = 'result.db'\n",
        "\n",
        "    # delete result.db\n",
        "\n",
        "    # if os.path.exists(db_file):\n",
        "    #   os.remove(db_file)\n",
        "\n",
        "    # Connect to the SQLite database or create a new one\n",
        "    conn = sqlite3.connect(db_file)\n",
        "\n",
        "    # Create a cursor object to execute SQL queries\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create a table for storing file names and results, with file_name being a PRIMARY KEY\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS results (file_name TEXT PRIMARY KEY, result TEXT, date DATE)''')\n",
        "\n",
        "    newModel = tf.keras.models.load_model('/content/drive/MyDrive/code/model_up')  # Load the model\n",
        "    test_dir = \"/content/drive/MyDrive/document/test/driver_license\"\n",
        "    test_images = list(pathlib.Path(test_dir).rglob(\"*\"))\n",
        "    class_names = [ 'driver_license','house_register','national_id_card','passport']\n",
        "\n",
        "    for image_path in test_images:\n",
        "        predicted_class = classify_image(image_path, newModel, class_names)\n",
        "        image_name = os.path.basename(image_path)\n",
        "        print(f\"The image {image_name} is classified as {predicted_class}\")\n",
        "        c.execute(\"INSERT OR REPLACE INTO results VALUES (?, ?, ?)\", (str(image_name), predicted_class, date.today()))\n",
        "        conn.commit()\n",
        "\n",
        "    # Define the SQL query to retrieve all records from the \"results\" table\n",
        "    qry = \"SELECT * FROM results\"\n",
        "\n",
        "    # Read the query result into a DataFrame\n",
        "    df = pd.read_sql_query(qry, conn)\n",
        "\n",
        "    # Close the cursor and connection\n",
        "    c.close()\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBuUnq9L1bbX"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for image_path in test_images:\n",
        "#     predicted_class = classify_image(image_path, newModel, class_names)\n",
        "#     image_name = os.path.basename(image_path)\n",
        "\n",
        "#     # Load and display the image\n",
        "#     img_display = plt.imread(image_path)\n",
        "#     plt.imshow(img_display)\n",
        "#     plt.title(f\"{image_name} -> {predicted_class}\")\n",
        "#     plt.axis('off')  # Don't show axis values\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BourGH6UK3yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77df9dc4-1f25-48b6-a542-85fbe81d6acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  file_name          result        date\n",
            "0   dl1.jpg  driver_license  2023-08-09\n",
            "1   dl2.JPG  driver_license  2023-08-09\n",
            "2   dl3.JPG  driver_license  2023-08-09\n",
            "3   dl4.JPG  driver_license  2023-08-09\n",
            "4   dl5.JPG  driver_license  2023-08-09\n",
            "5   dl6.JPG  driver_license  2023-08-09\n",
            "6   dl7.JPG  driver_license  2023-08-09\n",
            "7   dl9.JPG  driver_license  2023-08-09\n",
            "8  dl10.JPG  driver_license  2023-08-09\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect('result.db')\n",
        "\n",
        "# Define the SQL query\n",
        "qry = \"SELECT * FROM results\"\n",
        "\n",
        "# Read the query result into a DataFrame\n",
        "df = pd.read_sql_query(qry, conn)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "# Display the first few records of the DataFrame\n",
        "print(df.head(100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}